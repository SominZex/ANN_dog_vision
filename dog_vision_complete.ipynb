{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAB0lnRA37GS"
   },
   "source": [
    "# End to End Image Classification Model\n",
    "\n",
    "This notebook builds an end-to-end dog classification machine learning model using TensorFlow and TensorFlow Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "Identifying Dog Breed\n",
    "\n",
    "The goal is to develop a model that can identify the breed of a dog from a photograph.\n",
    "\n",
    "## 2. Data\n",
    "\n",
    "The data is collected from Kaggle: [Dog Breed Identification](https://www.kaggle.com/c/dog-breed-identification/data).\n",
    "\n",
    "## 3. Model Evaluation\n",
    "\n",
    "The model evaluation will be based on a file containing prediction probabilities for each dog breed.\n",
    "\n",
    "## 4. Features\n",
    "\n",
    "Some information about our dataset:\n",
    "- We are dealing with unstructured data (images), so we will likely use deep learning/transfer learning techniques.\n",
    "- The dataset contains over 10,000 images in the training set and over 10,000 images in the test set (these images are unlabeled as they are the ones we need to predict)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mounting Google Drive & Importing necessary libraries\n",
    "1. Mounting google drive\n",
    "2. Import libraries\n",
    "3. Checking GPU\n",
    "4. Init GPU\n",
    "5. Wamup the GPU and CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qA7V5V0nqQb",
    "outputId": "71fd6736-3cf4-419b-98a4-ebf59cf6ab03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive to access data (if running on Google Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNQCJymqufIy",
    "outputId": "a8113dc3-52a0-4412-d916-87291823ab8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "0.16.1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Check versions\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"TensorFlow Hub Version: {hub.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU\", \"Available\" if tf.config.list_physical_devices(\"GPU\") else \"Not Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "def cpu():\n",
    "  with tf.device('/cpu:0'):\n",
    "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "\n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Exploration\n",
    "1. Loading Image Data\n",
    "2. Displaying Sample Images\n",
    "3. Exploring Image Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Exploring Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_lab = pd.read_csv('drive/MyDrive/Deep_Learning/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset statistics\n",
    "print(df_lab.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(df_lab.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the value counts of breeds\n",
    "breed_counts = df_lab['breed'].value_counts()\n",
    "print(breed_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the median of count of images by breed\n",
    "df_lab[\"breed\"].value_counts().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting grapph of count of images by breed\n",
    "df_lab[\"breed\"].value_counts().plot.bar(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying a sample image\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Function to display a random image\n",
    "def display_random_image(df, image_folder):\n",
    "    random_index = np.random.randint(len(df))\n",
    "    image_id = df[\"id\"][random_index]\n",
    "    breed = df[\"breed\"][random_index]\n",
    "    image_path = f\"{image_folder}/{image_id}.jpg\"\n",
    "    display(Image(filename=image_path))\n",
    "    print(f\"Image ID: {image_id}, Breed: {breed}\")\n",
    "\n",
    "# Display a random image from the dataset\n",
    "display_random_image(df_labels, \"drive/MyDrive/Deep_Learning/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Image Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display specific image by index\n",
    "index = 8000\n",
    "image_id = df_labels[\"id\"][index]\n",
    "breed = df_labels[\"breed\"][index]\n",
    "image_path = f\"drive/MyDrive/Deep_Learning/train/{image_id}.jpg\"\n",
    "display(Image(filename=image_path))\n",
    "print(f\"Image ID: {image_id}, Breed: {breed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "1. Extracting Labels\n",
    "2. Checking Data Integrity\n",
    "3. Converting Labels to Boolean Values\n",
    "4. Preprocessing Images\n",
    "5. Creating Data Pipeline\n",
    "6. Vizualizing Data Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numpy array\n",
    "labels = df_labels[\"breed\"].to_numpy()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the number of labels matches the number of filenames\n",
    "if len(labels) == len(df_labels[\"id\"]):\n",
    "    print(\"Number of labels matches number of filenames\")\n",
    "else:\n",
    "    print(\"Numbers do not match\")\n",
    "\n",
    "# Check the length of labels\n",
    "print(f\"Number of labels: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Labels to Boolean Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique breed names\n",
    "unique_breeds = np.unique(labels)\n",
    "print(f\"Unique breeds: {unique_breeds}\")\n",
    "\n",
    "# Convert labels to boolean values\n",
    "bool_labels = [label == unique_breeds for label in labels]\n",
    "print(f\"Boolean labels: {bool_labels[:10]}\")\n",
    "\n",
    "# Verify the length of boolean labels\n",
    "print(f\"Number of boolean labels: {len(bool_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheking example boolean label and its occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[0]) #original label\n",
    "print(np.where(unique_breed==labels[0])) #index where label occured\n",
    "print(bool_labels[0].argmax()) #index where label occurs in boolean array\n",
    "print(bool_labels[0].astype(int)) #there will be 1 where the label occured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Images\n",
    "1. Loading and Resizing Images\n",
    "2. Normalizing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image size\n",
    "img_size = 224\n",
    "\n",
    "# Define function to preprocess images\n",
    "def preprocess_image(image_path, img_size=img_size):\n",
    "    # Read the image file\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode the image to a tensor\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "     #convert the color channel value form 0 - 255  to 0 - 1 values\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # resizing images\n",
    "    image = tf.image.resize(image, size=[img_size, img_size])\n",
    "    # Normalize the image to the range [0, 1]\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "# Test the function with a sample image\n",
    "sample_image_path = f\"drive/MyDrive/Deep_Learning/train/{df_labels['id'][0]}.jpg\"\n",
    "sample_image = preprocess_image(sample_image_path)\n",
    "print(sample_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Pipeline\n",
    "1. Creating TensorFlow Dataset\n",
    "2. Batching and Prefetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a TensorFlow dataset from image paths and labels\n",
    "def create_dataset(image_paths, labels, batch_size=32):\n",
    "    # Create a dataset of image paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    \n",
    "    # Function to load and preprocess images\n",
    "    def load_and_preprocess_image(path, label):\n",
    "        image = preprocess_image(path)\n",
    "        return image, label\n",
    "    \n",
    "    # Map the load_and_preprocess_image function to the dataset\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # Shuffle, batch, and prefetch the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=len(image_paths)).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Creating image paths\n",
    "image_paths = [f\"drive/MyDrive/Deep_Learning/train/{image_id}.jpg\" for image_id in df_labels['id']]\n",
    "\n",
    "# Creating the dataset\n",
    "train_dataset = create_dataset(image_paths, bool_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return a tuple of (image, label)\n",
    "def get_image_details(image_path, label):\n",
    "    image = preprocess_image(image_path)\n",
    "    return image, label\n",
    "\n",
    "# Function to create batches of data\n",
    "def create_data_batches(image_paths, labels, batch_size=32, valid_data=False):\n",
    "    if valid_data:\n",
    "        print(\"Creating validation data batches...\")\n",
    "    else:\n",
    "        print(\"Creating training data batches...\")\n",
    "    \n",
    "    # Create a dataset from the image paths and labels\n",
    "    data = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    \n",
    "    # Map the preprocessing function to the dataset\n",
    "    data = data.map(get_image_details, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # Batch and prefetch the dataset\n",
    "    data = data.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example image paths and labels\n",
    "image_paths = [f\"drive/MyDrive/Deep_Learning/train/{image_id}.jpg\" for image_id in df_labels['id']]\n",
    "\n",
    "# Create training and validation data batches\n",
    "train_data = create_data_batches(image_paths, bool_labels)\n",
    "val_data = create_data_batches(image_paths, bool_labels, valid_data=True)\n",
    "\n",
    "# Select one batch for visualization\n",
    "one_batch_train_data = train_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize data batches\n",
    "def visualize_data_batches(data_batch):\n",
    "    # Iterate over the batch\n",
    "    for images, labels in data_batch:\n",
    "        # Plot each image in the batch\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i in range(len(images)):\n",
    "            plt.subplot(4, 4, i + 1)\n",
    "            plt.imshow(images[i])\n",
    "            plt.title(unique_breeds[np.argmax(labels[i])])\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "# Visualize one batch of training data\n",
    "visualize_data_batches(one_batch_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building, Training, and Evaluation\n",
    "1. Create Model\n",
    "2. Creating Model Callbacks\n",
    "3. Training the Model\n",
    "4. Making Predictions\n",
    "5. Turning Predictions into Labels\n",
    "6. Unbatching Validation Data\n",
    "7. Plotting Predictions\n",
    "8. Predicting Top 10 Images\n",
    "9. Visualizing Both Graph and Image\n",
    "10. Saving and Exporting the Model\n",
    "11. Training a Big Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "# Setup input shape to the model\n",
    "img_size = 224  # Example size, adjust as needed\n",
    "input_shape = [None, img_size, img_size, 3]  # batch, height, width, color channels\n",
    "output_shape = len(unique_breeds)\n",
    "\n",
    "# Setup model URL from TensorFlow Hub (we're going to use already trained model)\n",
    "model_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\"\n",
    "\n",
    "# Creating a function to build the model\n",
    "def create_model(input_shape=input_shape, output_shape=output_shape, model_url=model_url):\n",
    "    print(\"Building model with:\", model_url)\n",
    "    model = tf.keras.Sequential([\n",
    "        hub.KerasLayer(model_url, input_shape=input_shape[1:]),\n",
    "        tf.keras.layers.Dense(units=output_shape, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# TensorBoard callback\n",
    "def create_tensorboard_callback():\n",
    "    logdir = os.path.join(\"drive/MyDrive/Deep_Learning/logs\",\n",
    "                          datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    return tf.keras.callbacks.TensorBoard(logdir)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5  # Example value, adjust as needed\n",
    "\n",
    "# Function to train the model\n",
    "def train_model():\n",
    "    \"\"\"Trains a given model and returns the trained model\"\"\"\n",
    "    model = create_model()\n",
    "    tensorboard = create_tensorboard_callback()\n",
    "    \n",
    "    # Fitting the model\n",
    "    model.fit(x=train_data,\n",
    "              epochs=num_epochs,\n",
    "              validation_data=val_data,\n",
    "              validation_freq=1,\n",
    "              callbacks=[tensorboard, early_stopping])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on validation data\n",
    "predictions = model.predict(val_data, verbose=1)\n",
    "\n",
    "# Example to view predictions\n",
    "index = 0\n",
    "print(predictions[index])\n",
    "print(f\"Max Value: {np.max(predictions[index])}\")\n",
    "print(f\"Sum: {np.sum(predictions[index])}\")\n",
    "print(f\"Max Index: {np.argmax(predictions[index])}\")\n",
    "print(f\"Predicted Label: {unique_breeds[np.argmax(predictions[index])]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning Predictions into Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predicted label\n",
    "def get_pred_label(prediction_probabilities):\n",
    "    \"\"\"Turning the array of prediction probability into image label\"\"\"\n",
    "    return unique_breeds[np.argmax(prediction_probabilities)]\n",
    "\n",
    "pred_label = get_pred_label(predictions[21])\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbatching Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to unbatch data\n",
    "def unbatch(data):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for image, label in data.unbatch().as_numpy_iterator():\n",
    "        images.append(image)\n",
    "        labels.append(unique_breeds[np.argmax(label)])\n",
    "    return images, labels\n",
    "\n",
    "val_images, val_labels = unbatch(val_data)\n",
    "print(val_images[0], val_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot predictions\n",
    "def plot_pred(prediction_probabilities, labels, images, n=0):\n",
    "    pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]\n",
    "    pred_label = get_pred_label(pred_prob)\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # Red for wrong and green for right prediction\n",
    "    color = \"green\" if pred_label == true_label else \"red\"\n",
    "    plt.title(\"{} {:2.0f}% {}\".format(pred_label, np.max(pred_prob)*100, true_label), color=color)\n",
    "\n",
    "# Visualizing predictions\n",
    "plot_pred(prediction_probabilities=predictions, labels=val_labels, images=val_images, n=119)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Top 10 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot top 10 predictions\n",
    "def plot_pred_conf(prediction_probabilities, labels, unique_breed, n=1):\n",
    "    pred_prob = prediction_probabilities[n]\n",
    "    true_label = labels[n]\n",
    "    \n",
    "    # 10 prediction confidence index\n",
    "    top_10_pred = pred_prob.argsort()[-10:][::-1]\n",
    "    \n",
    "    # 10 prediction confidence value\n",
    "    top_10_value = pred_prob[top_10_pred]\n",
    "    \n",
    "    # 10 prediction label\n",
    "    top_10_label = unique_breeds[top_10_pred]\n",
    "    \n",
    "    # Plotting\n",
    "    top_plot = plt.bar(np.arange(len(top_10_label)), top_10_value, color='grey')\n",
    "    plt.xticks(np.arange(len(top_10_label)), labels=top_10_label, rotation='vertical')\n",
    "    \n",
    "    # Coloring\n",
    "    if true_label in top_10_label:\n",
    "        top_plot[np.argmax(top_10_label == true_label)].set_color('green')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_pred_conf(prediction_probabilities=predictions, labels=val_labels, unique_breeds=unique_breeds, n=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Both Graph and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_multiplier = 30\n",
    "num_rows = 3\n",
    "num_cols = 2\n",
    "num_images = num_rows * num_cols\n",
    "plt.figure(figsize=(10*num_cols, 5*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_pred(prediction_probabilities=predictions, labels=val_labels, images=val_images, n=i+i_multiplier)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_pred_conf(prediction_probabilities=predictions, labels=val_labels, unique_breeds=unique_breeds, n=i+i_multiplier)\n",
    "\n",
    "plt.tight_layout(h_pad=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Exporting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def save_model(model, suffix=None):\n",
    "    model_dir = os.path.join(\"drive/MyDrive/Deep_Learning/models\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    model_path = model_dir + \"-\" + suffix + \".h5\"\n",
    "    model.save(model_path)\n",
    "    return model_path\n",
    "\n",
    "# Function to load a trained model\n",
    "def load_model(model_path):\n",
    "    print(f\"Loading Saved Model: {model_path}\")\n",
    "    model = tf.keras.models.load_model(model_path, custom_objects={\"KerasLayer\": hub.KerasLayer})\n",
    "    return model\n",
    "\n",
    "# Saving the model\n",
    "save_model(model, suffix=\"1000_images_mobileNet_Adam\")\n",
    "\n",
    "# Loading the trained model\n",
    "loaded_model = load_model(\"drive/MyDrive/Deep_Learning/models/20240530-17441717091054-1000_images_mobileNet_Adam.h5\")\n",
    "\n",
    "# Evaluating loaded model\n",
    "loaded_model.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Big Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data batch with the full dataset\n",
    "full_data = create_data_batches(image_paths, bool_labels)\n",
    "print(\"Creating training data batches...\")\n",
    "\n",
    "# Create a model for full dataset\n",
    "full_model = create_model()\n",
    "full_tensorboard_callback = create_tensorboard_callback()\n",
    "full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=3)\n",
    "full_model.fit(x=full_data, epochs=num_epochs, callbacks=[full_tensorboard_callback, full_model_early_stopping])\n",
    "\n",
    "# Saving full model\n",
    "save_model(full_model, suffix=\"full_image_reco_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
