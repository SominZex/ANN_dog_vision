{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAB0lnRA37GS"
   },
   "source": [
    "# End to End Image Classification Model\n",
    "\n",
    "This notebook builds an end-to-end dog classification machine learning model using TensorFlow and TensorFlow Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Statement\n",
    "\n",
    "Identifying Dog Breed\n",
    "\n",
    "The goal is to develop a model that can identify the breed of a dog from a photograph.\n",
    "\n",
    "## 2. Data\n",
    "\n",
    "The data is collected from Kaggle: [Dog Breed Identification](https://www.kaggle.com/c/dog-breed-identification/data).\n",
    "\n",
    "## 3. Model Evaluation\n",
    "\n",
    "The model evaluation will be based on a file containing prediction probabilities for each dog breed.\n",
    "\n",
    "## 4. Features\n",
    "\n",
    "Some information about our dataset:\n",
    "- We are dealing with unstructured data (images), so we will likely use deep learning/transfer learning techniques.\n",
    "- The dataset contains over 10,000 images in the training set and over 10,000 images in the test set (these images are unlabeled as they are the ones we need to predict)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mounting Google Drive & Importing necessary libraries\n",
    "1. Mounting google drive\n",
    "2. Import libraries\n",
    "3. Checking GPU\n",
    "4. Init GPU\n",
    "5. Wamup the GPU and CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qA7V5V0nqQb",
    "outputId": "71fd6736-3cf4-419b-98a4-ebf59cf6ab03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive to access data (if running on Google Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNQCJymqufIy",
    "outputId": "a8113dc3-52a0-4412-d916-87291823ab8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "0.16.1\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Check versions\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"TensorFlow Hub Version: {hub.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU\", \"Available\" if tf.config.list_physical_devices(\"GPU\") else \"Not Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "def cpu():\n",
    "  with tf.device('/cpu:0'):\n",
    "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "\n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Exploration\n",
    "1. Loading Image Data\n",
    "2. Displaying Sample Images\n",
    "3. Exploring Image Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Exploring Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_lab = pd.read_csv('drive/MyDrive/Deep_Learning/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset statistics\n",
    "print(df_lab.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(df_lab.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the value counts of breeds\n",
    "breed_counts = df_lab['breed'].value_counts()\n",
    "print(breed_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the median of count of images by breed\n",
    "df_lab[\"breed\"].value_counts().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting grapph of count of images by breed\n",
    "df_lab[\"breed\"].value_counts().plot.bar(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying a sample image\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Function to display a random image\n",
    "def display_random_image(df, image_folder):\n",
    "    random_index = np.random.randint(len(df))\n",
    "    image_id = df[\"id\"][random_index]\n",
    "    breed = df[\"breed\"][random_index]\n",
    "    image_path = f\"{image_folder}/{image_id}.jpg\"\n",
    "    display(Image(filename=image_path))\n",
    "    print(f\"Image ID: {image_id}, Breed: {breed}\")\n",
    "\n",
    "# Display a random image from the dataset\n",
    "display_random_image(df_labels, \"drive/MyDrive/Deep_Learning/train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Image Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display specific image by index\n",
    "index = 8000\n",
    "image_id = df_labels[\"id\"][index]\n",
    "breed = df_labels[\"breed\"][index]\n",
    "image_path = f\"drive/MyDrive/Deep_Learning/train/{image_id}.jpg\"\n",
    "display(Image(filename=image_path))\n",
    "print(f\"Image ID: {image_id}, Breed: {breed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "1. Extracting Labels\n",
    "2. Checking Data Integrity\n",
    "3. Converting Labels to Boolean Values\n",
    "4. Preprocessing Images\n",
    "5. Creating Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numpy array\n",
    "labels = df_labels[\"breed\"].to_numpy()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the number of labels matches the number of filenames\n",
    "if len(labels) == len(df_labels[\"id\"]):\n",
    "    print(\"Number of labels matches number of filenames\")\n",
    "else:\n",
    "    print(\"Numbers do not match\")\n",
    "\n",
    "# Check the length of labels\n",
    "print(f\"Number of labels: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Labels to Boolean Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique breed names\n",
    "unique_breeds = np.unique(labels)\n",
    "print(f\"Unique breeds: {unique_breeds}\")\n",
    "\n",
    "# Convert labels to boolean values\n",
    "bool_labels = [label == unique_breeds for label in labels]\n",
    "print(f\"Boolean labels: {bool_labels[:10]}\")\n",
    "\n",
    "# Verify the length of boolean labels\n",
    "print(f\"Number of boolean labels: {len(bool_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheking example boolean label and its occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[0]) #original label\n",
    "print(np.where(unique_breed==labels[0])) #index where label occured\n",
    "print(bool_labels[0].argmax()) #index where label occurs in boolean array\n",
    "print(bool_labels[0].astype(int)) #there will be 1 where the label occured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Images\n",
    "1. Loading and Resizing Images\n",
    "2. Normalizing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    # Read the image file\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode the image to a tensor\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    # Resize the image to the required size\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    # Normalize the image to the range [0, 1]\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "# Test the function with a sample image\n",
    "sample_image_path = f\"drive/MyDrive/Deep_Learning/train/{df_labels['id'][0]}.jpg\"\n",
    "sample_image = preprocess_image(sample_image_path)\n",
    "print(sample_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Pipeline\n",
    "1. Creating TensorFlow Dataset\n",
    "2. Batching and Prefetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a TensorFlow dataset from image paths and labels\n",
    "def create_dataset(image_paths, labels, batch_size=32):\n",
    "    # Create a dataset of image paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    \n",
    "    # Function to load and preprocess images\n",
    "    def load_and_preprocess_image(path, label):\n",
    "        image = preprocess_image(path)\n",
    "        return image, label\n",
    "    \n",
    "    # Map the load_and_preprocess_image function to the dataset\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    # Shuffle, batch, and prefetch the dataset\n",
    "    dataset = dataset.shuffle(buffer_size=len(image_paths)).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Creating image paths\n",
    "image_paths = [f\"drive/MyDrive/Deep_Learning/train/{image_id}.jpg\" for image_id in df_labels['id']]\n",
    "\n",
    "# Creating the dataset\n",
    "train_dataset = create_dataset(image_paths, bool_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create batches of data\n",
    "def create_data_batches(image_paths, labels, batch_size=32, valid_data=False):\n",
    "    if valid_data:\n",
    "        print(\"Creating validation data batches...\")\n",
    "    else:\n",
    "        print(\"Creating training data batches...\")\n",
    "    \n",
    "    # Create a dataset from the image paths and labels\n",
    "    data = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    \n",
    "    # Map the preprocessing function to the dataset\n",
    "    data = data.map(get_image_details, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # Batch and prefetch the dataset\n",
    "    data = data.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return a tuple of (image, label)\n",
    "def get_image_details(image_path, label):\n",
    "    image = preprocess_image(image_path)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example image paths and labels\n",
    "image_paths = [f\"drive/MyDrive/Deep_Learning/train/{image_id}.jpg\" for image_id in df_labels['id']]\n",
    "\n",
    "# Create training and validation data batches\n",
    "train_data = create_data_batches(image_paths, bool_labels)\n",
    "val_data = create_data_batches(image_paths, bool_labels, valid_data=True)\n",
    "\n",
    "# Select one batch for visualization\n",
    "one_batch_train_data = train_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize data batches\n",
    "def visualize_data_batches(data_batch):\n",
    "    # Iterate over the batch\n",
    "    for images, labels in data_batch:\n",
    "        # Plot each image in the batch\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        for i in range(len(images)):\n",
    "            plt.subplot(4, 4, i + 1)\n",
    "            plt.imshow(images[i])\n",
    "            plt.title(unique_breeds[np.argmax(labels[i])])\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "# Visualize one batch of training data\n",
    "visualize_data_batches(one_batch_train_data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
